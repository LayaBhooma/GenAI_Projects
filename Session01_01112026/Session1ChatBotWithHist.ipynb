{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: openai in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nirma\\training\\genai_projects\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "API_KEY: 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\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletionMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompt.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mget_gpt_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mget_gpt_response\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     15\u001b[39m conversation_history.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt})    \n\u001b[32m     16\u001b[39m response = client.chat.completions.create(\n\u001b[32m     17\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-5-nano\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     18\u001b[39m     messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     ]\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m response = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     24\u001b[39m conversation_history.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: response}) \n\u001b[32m     25\u001b[39m assistant_message = response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[31mTypeError\u001b[39m: 'ChatCompletionMessage' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#%pip install python-dotenv openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env.local\")\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"API_KEY: {my_api_key}\")\n",
    "#client = OpenAI(api_key=my_api_key)\n",
    "client = OpenAI(api_key=my_api_key)\n",
    "# The list will store the entire conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def get_gpt_response(prompt):\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": prompt})    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the user's questions concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    response = response.choices[0].message['content']\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response}) \n",
    "    assistant_message = response.choices[0].message.content\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message}) \n",
    "    return assistant_message\n",
    "while True:\n",
    "    prompt = input(\"What is your question, (or type 'exit' to quit): \") \n",
    "    if prompt.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "    print(f\"Response: {get_gpt_response(prompt)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
